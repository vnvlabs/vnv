


\section{Significance, Background, and Technical Approach}

\subsection{Identification and Significance of the Innovation}
\label{sec:identification}
%% {\em Define the specific technical problem or opportunity addressed by
%%  your application.  Provide enough background information so that the
%%  importance of the problem/opportunity is clear.  Indicate the overall
%%  technical approach to the problem/opportunity and the part that the
%%  proposed research plays in providing needed results.}

RNET Technologies Inc. (RNET) in Dayton, OH and Oak Ridge National Laboratory 
(ORNL) are responding to 2019 DOE SBIR/STTR Phase II Release 2 
(DE-FOA-0001976). This proposal is for a Phase II contract in succession to an 
initial Phase I contract (Contract \#: DE-SC0018728) awarded for topic DOE 
SBIR/STTR Topic 30d (Modeling and Simulation). Based on a prototype developed in Phase I, RNET and ORNL are proposing the development of VnV: a self-documenting 
testing framework for in-situ solution verification and validation in high performance computing applications. In this proposal we will highlight the need for, and the tremendous value of, 
the proposed tool in high performance numerical simulations like those in the NEAMS toolkit. The goal of the proposed Phase II project will be to develop a production quality implementation of the VnV framework that can deliver that value to a broad spectrum of numerical simulation applications. 

The overarching goal of the proposed VnV toolkit is to facilitate the development of \emph{explainable} numerical simulations that provide 
the user with not only the final solution, but also an explanation of how the solution was obtained and why it can be trusted. In doing so, the VnV toolkit will streamline the process of solution verification and validation for developers and end-users alike. RNET has extensive SBIR experience in various aspects of High Performance Computing such as performance optimization of numerical softwares and libraries, development of fine-grained power monitoring tools for HPC infrastructure, and the development of software usability tools the enhance the user experience of simulations. For example, RNET is currently developing a machine learning based plugin for automated solver selection in NEAMS tools and a cloud based workflow management tool that allows users to design, execute and visualize simulations on remote machines through a standard web browser. Dr. Watson (ORNL) has extensive experience developing parallel debugging software for high performance computing resources. He ...\rnetcomment{@Greg}. We believe that RNETs experience developing advanced numerical software for the HPC community combined with Dr. Watson's experience with scientific software engineering uniquely qualifies our team to fully develop and commercialize the VnV framework.

\subsubsection{Identification and Significance}
\label{intro}
Numerical modeling and simulation (\MS) is almost always more economical than live testing and full scale prototypes; a fact that has led to wide scale use of \MS in the design of products ranging from \$10 polycarbonate kids toys, up to solar panels, airplane wings and nuclear reactors. With access to large-scale computational resources at an all time high, and with exascale computing resources on the horizon; the role of numerical \MS in the design of next generation technologies is only expected to increase. However, numerical simulations are, by definition, an \emph{approximation} to a real world physical system. As such, it is important that this increased reliance on simulated tests is accompanied by concerted effort to ensure simulations are fit for the intended purpose.  According the the DoD best practices guide, verification and validation (\VV) of a code should be performed when  \emph{...the risk of making an incorrect decision based on a simulation outweighs the time and cost of performing \VVA to ensure that a simulation produces results that are sufficiently accurate and reliable.} One only needs to look to the Sleipner platform accident, where an offshore platform collapsed due to failures in the finite element simulation, to get an idea of the devastating consequences poorly verified numerical simulations can have on business, the environment and society.

In recent years, the DOE and other government agencies has been pushing for an increased rate of transfer of technology between government funded research institutions (both at national laboratories and academic institutions) and industry. For numerical simulation technologies, this push for technology transfer often manifests as the release of software using popular open-source licenses like the LGPL or the MIT. While the technical details of these open source licenses vary, they all contain language to the effect of ``this software is released as-is with no guarantee that the software is fit for the intended purpose'', even in cases where the software has been rigorously verified and validated throughout the development life-cycle. The result of this is that, while the developer should make every effort to ensure the code is fit for the intended purpose, it is ultimately the responsibility of the end user to ensure the inputs provided to the simulation packages lie within the scope of the applications capabilities, and that the solution obtained is an accurate representation of the physical model of interest to them. From a more practical standpoint, the direct costs of a design failure, be it time, money, or loss of life, fall squarely on the shoulders on the products developer, and any attempt to shift the blame to the developers of simulation library $X$ will likely fall of deaf ears.  

On the part of the DOE, legal indemnification from simulation errors make good business sense; however, the inability to explicitly state that these open source codes act as intended is significantly hindering the uptake and acceptance of the simulation codes in industry; primarily because the costs and depth of knowledge associated with performing the required \VV are impractical when compared to commercial tools. To ensure effective transition of the nations numerical simulation technologies, there is a significant need for the development of tools that aid in the end-user \VV of numerical simulations. While it might be legally impractical to unilaterally assert the accuracy of an open-source simulation, these tools should go to every length to equip the end user with the functionality and information required to determine if the given simulation is a faithful representation of the physical system of interest to them.   

End-user \VV is of particular importance in the DOE Nuclear Energy Advanced Modeling and Simulation (NEAMS) program. NEAMS is developing predictive models for the advanced nuclear reactor and fuel cycle systems using leading edge computational methods and high performance computing technologies \cite{NEAMS}. An important objective of the NEAMS program is to enable widespread use among the industry, academia, and regulatory communities\cite{NEAMS}. This objective has lead to the 
development of the NEAMS workbench, which has significantly increased the usability and user experience of the NEAMS tools. The NEAMS group has placed a significant emphasis on \VV in the NEAMS toolkit, as outlined in the NEAMS \VV plan (version 0) \cite{}. The result of this is that the NEAMS tools have integrated functionality for input validation (through the workbench and MOOSE), for performing mesh refinement and method of manufactured solution analysis (through MOOSE) and for completing uncertainty quantification and sensitivity analysis with DAKOTA (through the workbench \cite{}). Despite this, the complex nature of the codes, combined with the expert knowledge required to set up \VV testing and the infinite array of input parameters available in some tools (PETSc, the linear solver package used in MOOSE has thousands of configuration options alone) makes setting up robust end-user \VV an overwhelming task for all but the most expert users.   

In order to address the difficulties with end user solution verification in the NEAMS toolkit, and numerical simulation as a whole, RNET Technologies Inc. (RNET) and Oak Ridge National Laboratory (ORNL) are proposing the development of the VnV framework; a C/C++ software package designed to streamline the process of verifying and validating a numerical simulation using industry best practices. In particular, the framework will focus on providing 
robust functionality for the \VV of solutions obtained from general purpose numerical simulation packages (e.g., MOOSE, PETSc, libMesh, Fenics, OpenFoam, etc). To do this, the framework will facilitate the development of \emph{explainable} numerical simulations that, in addition to producing a simulation result, produce a detailed report outlining how the solution was obtained and why it should be trusted. These detailed simulation reports will dramatically reduce the burden of full scale \VV for end users in industry that would otherwise not have the time or expertise to complete through \VV. Overall, the explainable numerical simulations promoted by the proposed framework will increase the appeal the DOEs simulation technologies in industry, thereby increasing overall technology transfer and helping ensure the nation benefits from its significant investment in advanced numerical simulation technologies.


\subsubsection{Product Overview and Technical Approach}

Put simply, \VV is the process of ensuring a simulation is fit for its intended purpose. More precisely, Verification is the process of ensuring that a simulation is a faithful representation of the developers conceptual description and specifications (did I build the thing right?), and Validation is the process of determining the degree to which a model is an accurate representation of the physical model from the perspective of the intended use case (did I build the right thing?) \cite{DOD-VVA}. It is a long held belief of the software development community that the sooner a error is detected, the cheaper it is to fix. The same is true for numerical \MS and as such, \VV should be an integrated component of the software development life-cycle. Some examples of the tasks involved in the \VV of a numerical simulation include:

\begin{itemize}
 \item Development of a \VV plan
 \item Implementation of software development best practices (e.g. version control, unit and regression testing, code reviews, etc.)
 \item Mathematical and algorithmic testing (convergence analysis, mesh refinement studies, method of manufactured solutions, etc.)
 \item Development of a robust benchmark testing suite
 \item Uncertainty quantification and sensitivity analysis
 \item Comparison of simulation results with experimental data and results from third party simulations. 
 \item Review of the implementation and results by experts in the field
 \item Documentation of the \VV effort
\end{itemize}

In this proposal, we make the distinction between \VV a simulation package and the end-user \VV of a solution obtained using a simulation package. These two processes are not independent; indeed, \VV of a numerical simulation 
package almost always includes a set of verified and validated benchmark tests; likewise, the assertion that a package is verified and validated often forms the foundation of trust in the end user \VV of a new simulation. The focus of this proposal and the phase II project will be end-user validation; however, there is no reason that the functionality imparted by the proposed framework could not be used in the \VV process of the overall simulation package. They key issues associated with end-user verification and validation, and the approaches that the VnV framework will take to address them are:

\begin{itemize}
  \item{ \bf In-situ Testing And Analysis:} Unit tests are an extremely effective mechanism for ensuring a algorithm behaves as is expected. However, such testing 
  and an unavoidably discrete process that, by definition, cannot cover every possible outcome. This fact is particularly true for numerical algorithms, where even small 
  changes in the geometry of the mesh, or changes in a input parameter, can cause algorithms to diverge, or worse, converge to the incorrect solution. As such, a robust
  \VV report should not only include a description of unit tests completed, but also a detailed set of in-situ tests and assertions that describe the state and outcomes
  of important functions and variables as the simulation proceeds. Such tests allow us to verify the call graph of the simulation is as expected, and/or to detect the points
  in the simulation where the solution deviates from the expected result. Moreover, such tests allow for in-situ detection of unphysical characteristics (negative pressures, infinite 
  velocities,etc) and for in-situ comparisons with experimental data; both processes that can significantly cut down on the amount of data output required. To address this need, the VnV framework will include a sophisticated, cross library injection point system. This system will allow developers to declare locations and variables in the code where \VV testing can and should take place. In the simplest form, these injection points will act to trace the program, providing a coarse grained, highly descriptive report on the overall call graph flow of the program. However, the true power of the injection points is that the provide a location in the code for the runtime execution of external verification and validation tests. These tests are specified in external shared libraries and can be added or removed from the simulation at runtime using a XML configuration file. The tests could be as simple as asserting a value is positive, or as complicated as completing a statistical comparison between a distributed array and some experimental data. Such an approach keeps the overall code base clean, while still allowing for fine-grained for in-situ \VV testing that can be configured at runtime without the requirement for recompiling the source code. 
 \item {\bf Software Compartmentalization:} Modern numerical simulation design focuses on the idea of compartmentalization, whereby different components of the numerical simulation pipeline are split into separate libraries. For example, the NEAMS fuel performance code, BISON, uses the Multiphysics package MOOSE to define its physics, libMesh to define the finite element discretization and PETSc to solve the resulting linear system. This modularity allows researchers to do what they do best, while still profiting of the ever growing catalog of advanced numerical simulation toolkits. However, in the context of end-user \VV, this deep hierarchy of numerical packages makes \VV extremely difficult because, as stated above, the final solution should really be analyzed at each level; a process that requires a broad understanding each package that even the developers themselves may not possess. To address this issue, the injection point system defined above will include functionality for detecting and configuring in-situ \VV testing in any VnV equipt library linked to the final executable. That is to say, through a simple function call, the user will be able to obtain a  detailed list of all the injection points located across the range of libraries used in the executable. At runtime, the data collected from these cross library injection points will be assimilated into a single \VV report that covers the entire simulation hierarchy. In this way, the integration of \VV tooling into each specific library can be left up to the experts, while still allowing the end-user to easily verify functions and algorithms across multiple numerical libraries written in multiple languages. 
 \item {\bf Difficulty Designing \VV tests:} While the specific details of \VV vary from application to application, the marco scale algorithms used are relatively consistent, including;
 mesh refinement studies, using the method of manufactured solutions, sensitivity analysis, uncertainty quantification, error propagation. Many of these algorithms can, and in some 
 cases already have (see, NIMROD, DAKOTA, MASA) be implemented as black-box or near black box solutions. The VnV framework will look to capitalize on this fact by providing a set of robust,
 near black-box, \VV tools that can be integrated into codes with limited effort. Where possible, these tools will leverage existing black box solutions such as the MASA repository for MMS 
 analysis and the DAKOTA and NIMROD tools for UQ and SA. 
 \item{\bf \VV Testing Cost:}  Performing a large number of \VV tests in a distributed environment will be expensive, both computationally and due to the data movement required to deal with the domain decomposition employed by the application. Where possible, the VnV framework will offer functionality for offloading the execution of in-situ tests to external processes. The optimal approach to implementing such a feature is a key goal of the Phase II proposal, however, some ideas include using the streaming functionality of the ADIOS2 IO system to stream test data to an external \VV service and/or by using the MRNet or SNOball software. Offloading of the tests to an external server will significantly reduce the run time costs of in-situ \VV with the VnV framework; further increasing its utility in HPC applications.  
 \item{\bf Workflow Management:} A number of the most popular \VV analysis techniques, (e.g., UQ, SA, mesh refinement) require the core simulation to be run multiple times. In these cases, there is a significant potential for speeding up the verification and validation process through job based parallelism. As such, the VnV toolkit will support setting up job based parallelism using the common workflow language. 
 \item{\bf Documentation Generation:} With software packages under almost constant development, and new and improved packages being released on a regular basis, keeping an up-to-date \VV in an almost impossible. The VnV toolkit 
 will address this need by including automatic VnV report generation in the form of a server-less html web page visible in any modern web browser. The entire web page will be customizable using an extended markdown format and will include support for standard markdown formating, latex formating, images, videos, self-sorting tables, a two dimensional charts using charts.js  and three dimensional visualization with paraview, vtk.js and three.js. In each case, the tables and figures will be self plotting based on data extracted from the in-situ \VV testing. The format of the overall web report will follow the templates outlined in the DoD TODO VV template files. These templates have been used over many years in the \VVA of DoD simulations to great effect. Of course, additional custom templates will also be supported. 
 \end{itemize}

In summary, once integrated into an application, the VnV framework will provide a simple mechanism for creating self verifying, self describing, explainable numerical simulations. This will significantly reduce the burden associated with \VV for end user simulations, thereby increasing the usability of the tools for non-expert end-users. The framework will support a full range of in-situ \VV tests with functionality for offloading in-situ tests to external processors, as well as external \VV testing algorithms like UQ and mesh refinement studies with support for job based parallelism where needed. Finally, the package will include support for a highly customizable, automated \VV test generation for creating interactive and responsive web based \VV reports. A commercial license will be required to incorporate the VnV toolkit into for profit applications; however, after rigorous testing in real applications, the core functionality of the VnV toolkit will be released as open source for use in open-source, academic and enterprise applications, with RNET providing commercial support, training and integration services. 



\subsection{Anticipated Public Benefits}

The initial customers of the VnV framework will include the businesses and other institutions (e.g.,ANSYS, Cd-adapco, government labs, universities) that develop large-scale numerical modeling and simulations. To these customers, RNET will provide the training, support and integration services required to quickly and efficiently integrate the VnV framework into new and existing code bases, as well as contract based services for extending the toolkit to fit certain needs. For these customers, the benefits could include streamlining of the companies internal \VV practices, and an increase in the usability, reliability and confidence in their 
final product. However, the true beneficiaries of the VnV frameworks end-user \VV functionality are the users of the advanced numerical simulation products. By removing the burden associated with
\VV the VnV framework will ensure erroneous errors do not propagate into final designs, while also opening the door to using simulations tools for which \VV was previous infeasible. All in all, the VnV framework will 
afford researchers and engineers with the knowledge required to drive the next generation of technological advancements. 

\subsection{Phase I and Feasibility Demonstration}
