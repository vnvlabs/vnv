\section{Significance, Background, and Technical Approach}

\subsection{Identification and Significance of the Innovation}
\label{sec:identification}
%% {\em Define the specific technical problem or opportunity addressed by
%%  your application.  Provide enough background information so that the
%%  importance of the problem/opportunity is clear.  Indicate the overall
%%  technical approach to the problem/opportunity and the part that the
%%  proposed research plays in providing needed results.}

RNET Technologies Inc. (RNET) in Dayton, OH and Oak Ridge National Laboratory 
(ORNL) are responding to 2019 DOE SBIR/STTR Phase II Release 2 
(DE-FOA-0001976). This proposal is for a Phase II contract in succession to an 
initial Phase I contract (Contract \#: DE-SC0018728) awarded for topic DOE 
SBIR/STTR Topic 30d (Modeling and Simulation). Based on a prototype developed in Phase I, RNET and ORNL are proposing the development of VnV: a self-documenting 
testing framework for in-situ solution verification and validation in high performance computing applications. In this proposal we will highlight the need for, and the tremendous value of, 
the proposed tool in high performance numerical simulations like those in the NEAMS toolkit. The goal of the proposed Phase II project will be to develop a production quality implementation of VnV that is capable of 
delivering this value across a broad spectrum of numerical simulation applications. The overarching goal of the proposed VnV toolkit is to facilitate the development of \emph{explainable} and \emph{verifiable} numerical simulations that provide 
the user with not only the final solution, but also an explanation of how the solution was obtained and why it can be trusted. In doing so, the VnV toolkit will streamline the process of solution verification and validation for developers and end-users alike, thereby increasing the value of advanced numerical simulation technologies in a broad range of academic and industrial applications.  

RNET has extensive SBIR experience in various aspects of High Performance Computing such as performance optimization of numerical softwares and libraries, development of fine-grained 
power monitoring tools for HPC infrastructure, and the development of software usability tools the enhance the user experience of simulations. For example, RNET is currently developing a machine learning based plugin for 
automated solver selection in NEAMS tools and a cloud based workflow management tool that allows users to design, execute and visualize simulations on remote machines through a standard web browser. Dr. Watson (ORNL) has extensive experience developing parallel debugging software for high performance computing resources. He ...\rnetcomment{@Greg}. We believe that RNETs experience developing advanced numerical software for the HPC community combined with Dr. Watson's experience with scientific software engineering uniquely qualifies our team to fully develop and commercialize the VnV framework.

\subsubsection{Identification and Significance}
\label{intro}

Numerical simulations are playing an ever increasing role in the research and development (R&D)
pipeline, with new and more powerful algorithms and packages being developed every year. When
numerical simulation codes are used to inform design real world products (e.g., nuclear reactors, space shuttles, airplanes, etc) erroneous
simulation errors can propagate into final designs causing issues that can be extremely expensive to fix, damage the
environment, and, in some unfortunate cases, result in loss of human life. In response to these concerns, there has been a concerted effort
to define a robust approach for ensuring that numerical simulation technologies are ``fit for the intended purpose``. 

The process for determining the suitability of a simulation for a given application is known as \emph{Verification and Validation} (V&V). The first V&V guideline was issued by the American Nuclear society in 1987. Further development of V&V guidelines was conducted by other professional organizations to provide industry-specific standards. In 1998, the American Institute of Aeronautics and Astronautics (AIAA) released its first modern standards document. Thereafter, the American Society of Mechanics Engineers (ASME) V&V Standards Committee issued documents related to fluid dynamics and solid mechanics, including
\begin{itemize}
 \item A Standard for Verification and Validation in Computational Fluid Dynamics and Heat Transfer, ASME V&V-20 (2009).
 \item An Illustration of the Concepts of Verification and Validation in Computational Solid Mechanics, ASME V&V-10.1 (2012).  
\end{itemize}
These guidelines have acted as references in numerous V&V plans across industry, including the DoD's rigorous VV&A (verification, validation and accreditation) plan and the NEAMS V&V plan.

The current best practice for the V&V of a numerical simulation package is a multi-faceted process that includes:
\begin{itemize}
 \item Rigorous application of software development best practices, including version control, code reviews, unit tests and automated regression testing. 
 \item Algorithm level techniques such as mesh refinement studies, the method of manufactured solutions and error propagation analysis
 \item Uncertainty quantification and sensitivity analysis. 
 \item Application to a suite of benchmark problems.
 \item Comparison with experimental results.
 \item Comparison with other results obtained from independent 3rd party codes.
 \item Review of the code and results by an expert in the field.
 \end{itemize}

While it is the responsibility of the program developers to ensure that their product is mathematically correct, free of programming errors, and in general, fit for the intended purpose; it is the primary responsibility of the end user to ensure the inputs provided to the verified and validated simulation lie within the scope of the applications capabilities and are an accurate representation of the physical model of interest. As with the V&V of the overall package, this requires an in-depth analysis of the underlying assumptions, algorithms and implementations used in generating the result. At a high level, this can includes tasks like mesh refinement studies, comparisons with experimental data and the assertion of physical properties (e.g., conservation of mass and momentum); but, it must also include a careful examination of the suitability, stability and accuracy of the internal algorithms (e.g., was an appropriate linear solver used and did it converge within the specified tolerance? Was the Jacobian well defined? Did the nonlinear solver converge to the global minimum? etc.). Such requirements break the encapsulation of the simulation, requiring a user to have an level of expertise regarding the internals of the code that would otherwise only be expected of the original developers. 

End-user V&V is of particular importance in the DOE Nuclear Energy Advanced Modeling and Simulation (NEAMS) program. NEAMS is developing predictive
models for the advanced nuclear reactor and fuel cycle systems using leading edge computational methods and high performance computing
technologies \cite{NEAMS}. An important objective of the NEAMS program is to enable widespread use among the industry, academia, and regulatory communities\cite{NEAMS}. This objective has lead to the 
development of the NEAMS workbench, which has significantly increased the usability and user experience of the NEAMS tools.

In order to address the difficulties with end user solution verification in the NEAMS toolkit and in the numerical simulation as a whole, RNET Technologies Inc. (RNET) and Oak Ridge National Laboratory (ORNL) are proposing the development of the VnV; a framework that facilitates the development of \emph{explainable} numerical simulations that provide the user with not only the result of the simulation, but also
with an explanation as to how the solution was obtained and why it should be trusted (the term ''explainable`` numerical simulations is a reference to the field of 
explainable artificial intelligence (XAI), where researchers are developing mechanisms for promoting trust in otherwise black-box AI algorithms). To do this, the VnV framework will provide developers with
a robust set of V&V tests  that, at runtime, can be inserted into developer specified injection points located at key points in the code. Users will be able to pick and choose which tests to run at which injection 
points using a YAML based configuration script loaded at runtime. The development of new, custom tests will require implementation of a simple test interface. At the completion of the simulation, the VnV framework with combine the test specific outputs with developer specified test description files to produce a responsive, server-less HTML web page that can be displayed in any web browser. This extremely detailed, highly customizable simulation report will include all the information required to verify the validity of the simulation, including information about the executable (version, shared libraries, machine configuration, etc), the inputs, the outputs, and a coarse grained call graph describing the injection points called and the tests executed. The target user for the V&V toolkit is the application end-user, however, there is no reason that the toolkit cannot be used by the developer in the benchmark tests used in the V&V of the overall package. The runtime configuration available in the toolkit also has significant use cases for parallel debugging; in fact, many of the statistical V&V tools that will be included in the framework were first developed for use in a relative parallel debugger \cite{}. In addition to injection point testing, the framework will include an interface for defining full simulation testing algorithms using the common work flow language. This interface will allow users to setup, execute and process tests that require the simulation to be run multiple times. This will include tools for automating mesh refinement studies and for setting up uncertainty quantification and sensitivity analysis routines (i.e., with NIMROD or DAKOTA). 

In summary, once integrated into an application, the VnV framework will provide a simple mechanism for creating self verifying, self describing, explainable numerical simulations. This will significantly reduce the burden associated with V&V for end user simulations, thereby increasing the usability of the tools for non-expert end-users. Although some NEAMS tools can already be configured to perform mesh refinement and MMS studies, and while one can configure UQ and sensitivity analysis testing though the workbench, there is still a significant benefit to integration of the VnV into the core tools used by NEAMS (Proteus, BISON,MOOSE,libMesh,PETSc, etc); not only because it provides a uniform interface for designing, running and reporting on all such tests, but also because the transparency afforded by the in-situ injection point testing mechanism promotes a level of trust in the solutions obtained from NEAMS tools that is not otherwise available in current commercial tools. 

\subsubsection{Product Overview and Technical Approach}

The VnV framework is a C/C++ API for facilitating the development of explainable numerical simulations in high performance computing applications. As shown in Figure~\ref{TODO}, the VnV framework 
can be considered in three main parts; the injection point framework, the workflow management system and the post-processor.

The VnV injection point system represents the core component of the overall framework. This system provides a simple mechanism for 
declaring, describing and locating key processes in the code that should be included in the final VnV report. To specify an injection
point, the user needs to provide a unique name (the full scope of the function containing the injection point is usually a good start), a 
markdown based description of the injection point for use in the final report, and a list of (in scope) variables and there associated types that 
will be available for inspection at the injection point. Once these requirements have been determined, inserting the injection point into the code
is as simple as including the appropriate header and calling the injection point function as shown in the following example. 

\begin{verbatim}

File: "test.c" 
 ...
 #include "vv-runtime.h" // Include the VV runtime header 
 ...
 class TestClass {
  public:
    int x;
    TestClass(int x_) : x(_x) {}
 }
 
 class ExampleClass {
 
    int x = 0;
    double y = 10;
    
    int function(TestClass ll) {

      // Add the injection point. 
      INJECTION_POINT("SampleInjectionPoint", int x, double y, TestClass cl) 

      return ll.x + x ;
    }
 }
 ...
\end{verbatim}

\begin{verbatim}
 File: "Injection_Points.yml
 ...
 SampleInjectionPoint:
   content: | 
      This function accepts a TestClass and return the value TestClass.x + this.x.
      This content supports all standard markdown formating. 
   parameters:
   - x : 
      type: int 
      description: x is a class variable  
  - y : 
      type: double
      description: y is another class variable
  - ll : 
      type TestClass 
      description: ll is the Test class passed to the function. 
\end{verbatim}

In that case, the injection point ''SampleInjectionPoint`` will be called each time the function ''function`` is called. 
The content for the injection point is stored  separate file, in this case, ''Injection_Points.yml``. This keeps the original
code clean, provides a simple mechanism for customization, and limits unnecessary string processing in the running simulation. 

A key issue 




In the simplest form, these injection points provide a mechanism for automatic generation of a coarse grained, user specified call graph for 
each simulation (i.e., the final report will be a coarse grained call tree where each leaf represents an injection point location.)However, 
the true power of the injection points is that they provide a location for injecting external V&V tests into the 
code. 




Inserting an injection
point into a code is as simple as including the VnV header file, inserting the call to the injection point library, and linking the 
library in the final executable. 



The VnV framework, the principal deliverable of the Phase II project, will be a software API that can be integrated into existing applications to facilitate 
the development of explainable numerical simulations. Figure~\ref{fig:outline} provides a high level overview of the framework and its functionality. 

At the heart of the framework are ''injection points`` that represent key locations in the code where V&V testing can place. A detailed description of the code can be specified for each injection point 
such that when a simulation is run, the injection point descriptions form a highly descriptive, coarse grained call trace of the simulation; the first step toward explainable numerical simulations. 

However, the true power of the injection points is that the provide a location in the code for the runtime execution of external verification and validation tests. These tests are specified in external shared 
libraries and can be added or removed from the simulation at runtime using a XML configuration file. The tests could be as simple as 
asserting a value is positive, up to a statistical comparison a distributed array against a experimental result. In addition to the code to be executed, each test is accompanied by 
a markdown based post-processing file that describes the details and results of the test. 

The VnV will also support external, full simulation testing mechanisms such as sensitivity analysis and mesh refinement studies using the common workflow language. In those cases, injection point based testing will still be useful for determining the validity of each of the full simulation testing runs. 

The final component of the V&V testing framework is automated report generation. This component of the toolkit takes the output of the VnV testing (an efficient ADIOS2 bp3 file) and the associated markdown specification files, and uses 
them to build a modern, responsive HTML/JS web page that can be displayed in any web browser, and/or hosted on a low-cost, server-less web hosting service (i.e., from an AWS S3 bucket). 




CloudBench is a hosted simulation environment for large scale numeric
simulations. CloudBench:NE is the application of CloudBench to Nuclear
Engineering simulation tools. The initial focus is on advanced reactor
tools, but CloudBench:NE will include support for advanced reactors
and light-water reactors simulation tools. CloudBench will augment
existing simulation, Integrated Development Environment, and workbench
tools being developed by the DOE and industry. It offers a complete
set of simulation management features for open source, government
sourced, and commercial simulation codes in a single integrated
workbench;

\begin{itemize}
\item interactive workflow management using a ``human-in-the-loop'' approach,
\item sharing of configurations, simulation output, and provenance on
  a per simulation or per project basis (ensuring that export control
  and license restricts are maintained),
\item hosted versions of advanced simulation codes (removing the need
  for the end user to perform the installation),
\item multi-simulation provenance history to allow simulations to be
  reconstructed, verified, or extended, and
\item remote access to simulation tools installed on Cloud and HPC
  resources.
\end{itemize}

Cloud Bench will allow the users to setup, launch/execute, and
visualize their simulations through a web-based workflow management
interface. The workflow management will allow outputs to be reused
(possibly after the output is translated and/or remeshed using
provided/built in or user generated/provided scripts) as input for
another simulation. References to the inputs, outputs, applications,
and workflow will be recorded. These records will provide provenance
for the set of simulations and allow the original results to be
regenerated for verification. The provenance also provides a known set
of working experiments that can be modified to support further
research. A user will be able to modify or extend a set of
experiments, and rerun the simulations to formulate new results.

The workflow management includes the ability to remotely manage job
execution and records with sufficient detail to provide provenance for the
simulation results. The provenance will include the simulation tools
used (including version and build information where applicable),
simulation parameters, input files, operating systems (and
environment), and hardware. The provenance provides
sufficient information to allow the simulations to be verified, but
also provides a basis for verifying the results using different
environments and inputs. This improves both the scientific
significance and the regulatory acceptance for nuclear simulation
results.

The provenance (input, output, and application information) can be
selectively shared with other users. This sharing will increase
collaboration and allow for regulatory agencies to get a precise
provenance on the applications, hardware, settings, and inputs.

In addition to provenance and data sharing, CloudBench provides a
front-end for third-party simulation tools and supports simulation
execution on local and remote (Cloud, HPC) resources.  This provides
access to preinstalled simulation tools without the burden of
installing the tools. Additionally, it provides easy access to public
and private HPC resources that are available to the user.

CloudBench is a scientific workflow framework that will increase the
usability, access, and value of numerical simulation tools. The online
CloudBench framework will be licensed to small/medium research sized
groups (start up companies, small government or academic labs, etc.)
on a seat-by-seat basis. The CloudBench server will also be licensed
to large organization for installation on private resources. This will
allow the organization to independently control access to simulation
software and datasets by hosting the server on internal
resources.

\subsection{Anticipated Public Benefits}
CloudBench users include businesses and other institutions (e.g.,
government research labs, universities, energy companies) that perform
large-scale numerical modeling and simulation using high performance
compute infrastructure (large super computers, small clusters, and
high-end workstations). The companies and government labs in the
numerical software development business (e.g., ANSYS, Cd-adapco, NEAMS
toolkit, CASL) continuously develop new tools that would benefit from
CloudBench.
%The businesses in cloud and high performance computing
%(e.g., NVidia, Intel, Amazon, Google) develop novel hardware and
%compute environments that must be supported by these applications and
%understood by users.
The benefits of CloudBench include the adoption of
new advanced simulation tools to improve product development and
product design.

%% CloudBench will facilitate wide adoption of high performance and
%% advanced government simulation codes (including NEAMS and associated
%% tools) by the academia, industry, and regulatory communities.  The
%% extreme benefit of this technology to the Nuclear Engineering
%% Community can not be overstated. A serious issue with the simulation
%% tools such as the NEAMS Toolkit is that its technology
%% \textbf{\textit{is too new to run on existing vendor and utility
%%     hardware}}. To wit, the NEAMS Toolkit is state of the art and many
%% industry tools can only be used on much older hardware and operating
%% systems which makes it impossible to compile the NEAMS
%% Toolkit. Usability (including workflow and provenance management) and
%% code access are limiting factors to adoption by third-party commercial
%% users.

The CloudBench differentiating factor is support for advanced open
source tools. The existing workbench and life cycle management tools
offered by traditional vendors support only the vendor's tools, and do
not include support for advanced open source simulation tools. Analysts
like Frost and Sullivan \footnote{``Global CAD and Modeling Software Market,'' Frost \& Sullivan, January 2013. [subscription required].} expect that efficient workflows
and access to codes will drive the adoption of next generation
simulation technology. Workflow and life cycle management tools are
not readily available for the state-of-the-art open source or
government license tools (e.g., the NEAMS and CASL). Therefore,
CloudBench will fulfill this role and provide much needed support for
these advanced simulation tools.

The initial release of CloudBench is CloudBench:NE (for Nuclear
Engineering) will be developed during Phase II. NE is an important
first niche market.  As the U.S. and the world ramp up to deploy new
advanced nuclear reactors, the ability to leverage advanced codes,
deploy on HPC and Cloud resources, and share experiments (for
collaborative and regulatory scenarios) is becoming critical to the
design and regulatory process. While the required workflow management
tools often exist in vendor simulation suites, they are missing from
open sourced and government codes.

This initial NE market includes United States companies such as GE
Hitachi, Westinghouse Electric Company, AREVA, Anatech, Nuscale,
Bechtel Marine Propulsion Corporation, Tennessee Valley Authority,
Studsvik Scandpower, Terrapower, Oklo, Starcore Nuclear,
FPoliSolutions, BWXT Technologies, Transatomic Power Corporation, X
Energy, Terrestrial Energy, Areva, and Flibe Energy.

\subsection{Phase I and Feasibility Demonstration}
\rnetprop{
RNET worked closely with Mr. Jay Billings from ORNL and the lead architect and 
principal investigator for the Eclipse Integrated Computational Environment 
(ICE) in prototyping CloudBench. ICE can interface to many different 
computational codes and NEAMS tools and its modular design allows us to 
separate various components in preparation for a web-based interface. Our 
approach has been to separate ICE into a front-end and back-end service (which 
would run on any compute, cloud or local instance). The front-end User 
Interface (UI) would be reimplemented using a web-based UI framework like 
Vaadin~\cite{vaadin}. This can connect to the back-end which will then allow 
simulations to be remotely executed. This requires the use of Remote OSGi 
services~\cite{osgi_ecf} which automatically distributes and proxies 
communication correctly. The back-end service is also referred to 
as the ``Core'', since it is developed from ICE's Core component.
}

In order to demonstrate feasibility of developing this product, RNET and ORNL 
have accomplished the following.

\begin{itemize}
\item \rnetprop{Prototyped a web-based UI for CloudBench using Vaadin and a system for 
choosing users to share data with and verifying appropriate notifications.}
\item \rnetprop{Resolved various technical challenges in isolating the Core, producing a 
standalone executable and setting it up to run as a Remote OSGi service, which 
is crucial towards being able to run on any server.}
\item \rnetprop{Validated the Remote OSGi connection and the ability to connect via an
EDEF File.}
\item \rnetprop{Ported user interface components for a Nek5000 form to the Vaadin 
interface.}
\end{itemize}

The following subsections provide details on the efforts to
demonstrate CloudBench feasibility.

\subsubsection{Validation of Remote Connection}
\rnetprop{
The Core component in ICE was setup to be a Remote OSGi service. The setup to 
test this is outlined in the Phase I Feasibility Report and is shown here in 
Figure~\ref{fig:test_remote_connections}.
}

\begin{figure}[thb]
\begin{center}
\leavevmode
\includegraphics[width=0.5\linewidth]{./narrative/figures/ice_core_connect.png}
\end{center}
\caption{Setup to test Remote Connections.}
\label{fig:test_remote_connections}
\end{figure}

%%REMOVE FOR SPACE
%% The remote connection is confirmed by the ImportRegistration event on the Host 
%% Windows machine as shown in Figure~\ref{fig:edef_connect}.

%% \begin{figure}[thb]
%% \begin{center}
%% \leavevmode
%% \includegraphics[width=0.7\linewidth]{./narrative/figures/edef_connect_cropped.png}
%% \end{center}
%% \caption{Import Registration on Host Machine.}
%% \label{fig:edef_connect}
%% \end{figure}

\subsubsection{Development of a CloudBench UI}
\rnetprop{Vaadin was chosen as the framework to develop a web-based interface. This was 
mainly due to the intuitive APIs Vaadin offers, the excellent Vaadin security model, as well as the automatic 
adjustments to the interface, based on the device being used to view it, such 
as smartphones and tablets~\cite{vaadin}. Upon successful user login, the 
CloudBench Dashboard is presented where the main area contains a panel with 
up-to-date relevant information on status of executed jobs 
%(\rnetcomment{Can add some panels here})
and notifications on shared output.}
%(\rnetcomment{Currently, only notification, no file sharing yet}).

\begin{figure}[!htb]
\begin{center}
\leavevmode
\includegraphics[width=0.7\linewidth]{./narrative/figures/cloudbench_dashboard_cropped.png}
\end{center}
\caption{CloudBench Dashboard.}
\label{fig:cbench_dashboard}
\end{figure}

\subsubsection{Simple Sharing and Notifications}
\rnetprop{We have designed CloudBench to support File Sharing among a group of Users. To 
facilitate this, the UI includes an ``search as you type'' box to pick out 
Users to share data with.}

\begin{figure}[!thb]
\begin{center}
\leavevmode
\includegraphics[width=0.7\linewidth]{./narrative/figures/cloudbench_sharedialog.png}
\end{center}
\caption{Selecting Users to Share With.}
\label{fig:share_users}
\end{figure}

In this case, we are sharing data with two test Users, ``Solomon Olsen'' and 
``Elvis Olsen''. When we login to one of their accounts, we can see that a 
notification from the originator (test User, ``Gabrielle Patel'') is visible 
and informs the User of the shared object.

\begin{figure}[!thb]
\begin{center}
\leavevmode
\includegraphics[width=0.9\linewidth]{./narrative/figures/eolsen_shared.png}
\end{center}
\caption{Share Notification for Chosen User.}
\label{fig:share_eolsen}
\end{figure}

\subsubsection{Headless Workflow Execution and Remote Job Execution}
\label{sec:remote_exec}
\rnetprop{ Mr. Billings and the ICE team worked with RNET personnel to modify the workflow 
engine in Eclipse ICE such that it can execute workflows headlessly as a Remote 
OSGi service. Although the ICE Core could be run headlessly before, it was by a 
different mechanism that did not meet the needs of the RNET team. Specifically, 
Remote OSGi services greatly simplify both memory management and communications 
between the service client and service provider while maintaining all of the 
normal advantages of an OSGi service that the original mechanism - a purely 
RESTful web service - lost. One other advantage of using the Remote OSGi 
service is that service discovery is simplified on small networks, which has 
great advantages in production deployment. This achievement simplifies workflow 
processing in CloudBench by removing the dependency on the ICE workbench, and 
RESTful service to provide a much simpler programming API. One major advantage 
of this is that workflow execution and job launch can now be performed on a 
server that communicates remotely with the CloudBench web client, while still 
maintaining all of the normal remote job execution capabilities in ICE. ICE's 
normal job launch framework supports local and remote job execution for several 
codes in NEAMS and offers full support for batch systems such as SLURM and PBS 
as well as parallel performance monitoring and remote debugging tools.}


\rnetprop{These updates were used to develop a working example of Vaadin with OSGi and 
implemented the Nek5000 parameters form in Vaadin as shown in 
Figure~\ref{fig:vaddin_nek5000}. This example demonstrates the feasibility of 
leveraging ICE's existing support for NEAMS tools in an easy, extendible way to 
support CloudBench.}

\begin{figure}[!thb]
\begin{center}
\leavevmode
\includegraphics[width=0.8\linewidth]{./narrative/figures/vaadin_nek5000_cropped.png}
\end{center}
\caption{Vaadin implementation of the Nek5000 form.}
\label{fig:vaddin_nek5000}
\end{figure}

\subsubsection{Advanced Visualization}
\label{id_advViz}
\rnetprop{In addition to the Phase I efforts, existing visualization work performed by the ICE team will be used in CloudBench and demonstrate feasibility of the proposed approach.
The Eclipse Advanced Visualization Project (EAVP) was originally developed as part of the NEAMS program to provide visualization support for 3D post-processing visualizations using VisIt and Paraview in tandem with ICE's workflow engine as well as to provide support for 2D graphing, and 3D mesh and geometry editing. This project was spun off separately from ICE several years ago to answer requests from users to use it outside of ICE and it has continued to grow on its own ever since. The project provides}
\begin{itemize}
\item \rnetprop{VisIt integration for working with mesh data}
\item \rnetprop{ParaView integration for working with mesh data}
\item \rnetprop{2D mesh editing support (for Nek5000)}
\item \rnetprop{3D mesh editing support (for surface meshes)}
\item \rnetprop{3D geometry editing}
\item \rnetprop{A full plotting library that supports remote updates and streaming}
\item \rnetprop{Support for remote renderers and multiple connections across VisIt and Paraview}
\end{itemize}


